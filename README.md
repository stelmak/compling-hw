# compling-hw
Учебный репозиторий по курсу "Компьютерная лингвистика". Стельмак Е. А.

Домашнее задание №2

Описание проекта:
    При работе над проектом было исследовано три различных способа токенизации: токенизация с помощью регулярных выражений и с помощью специально предназначенных для токенизации библиотек: NLTK (Natural Language Toolkit) и spaCy. Для работы соотвественно были установлены раннее упомянутые библиотеки. 

    Был создан модуль "tokenizer.py", в котором был создан класс "TextTokenizer" внутри которого находились следующие функции:
    1. Трансформер - не был в конечном результате использован

    2. simple_tokenize - на вход принимает текст, и токенизирует его с помощью регулярного выражения "\w+" и команды findall. Таким образом, такой токенайзер находит все повторения символа слова (буквы разного алфавита, цифры, но не знаки препинания) до пробела (первого символа не являющегося символом слова).

    3. nltk_tokenize - на вход принимает текст и токенизирует его с помощью команды .word_tokenize, которая выделяет каждое слово, цифру, а также знаки препинания. Также существует ещё команда .sent_tokenize, которая не просто разделяет текст на токены, но и также может разделить текст на предложения создав внутри списка подсписок для каждого предложения. Однако эта команда не была использована в этом модуле, поскольку входной текст не был достаточно большим, а также подобная структура усложнила бы вывод значений в словаре. 

    4. spacy_tokenize - на вход принимает текст и токенещирует его с помощью пайплайна, который предворительно заносится в переменную в нашем случае это переменная "nlp" и пайплайн "ru_core_news_sm". Для вывода отдельных токенов, необходимо с помощью цикла for и команды .text вывести каждый отдельно. Это необходимо поскольку при токенизации каждый токен является не в строкой, а хаш-значением результатом свёрстки, произведённой с целью экономии памяти. Для каждого токена также возможно выдать лемму и произвести тегирование, но в этом простом модуле это было бы излишне. 

    5. tokenize_all - в этой функции на вход принимается текст, над ним выполняются все выше описанные функции, а затем результат трёх токенизаций заноситься в словарь с соотвествующими способу токенизации названиями. 

    6. demo() - нужна для непосредственного тестирования модуля внутри самого модуля, то есть в случаях, когда не используется "demo.py"

Файл "demo.py" содержит в себе примерно такой же код, как и в функции demo(), однако другой текст для токенизации. Этот файл - скрипт для демонстрации работы модуля. 
    from tokenizer import TextTokenizer - из модуля "tokenizer" импортируется описанный выше класс TextTokenizer 

    def main():
        tokenizer = TextTokenizer() - к переменной присваивается класс

        sample_text = "Егор Крид — российский певец, рэпер, автор песен, актёр, ютубер, стример и телеведущий. Сольную карьеру начал в 2011 году под псевдонимом KReeD, сейчас выступает под именем Егор Крид. Является автором и исполнителем собственных песен. Заслуженный артист Республики Башкортостан!" - текст для токенизации

        results = tokenizer.tokenize_all(sample_text) - из класса в переменной tokenizer к тексту для токенизации применяется функция tokenize_all, которая содержит в себе три функции с разными методами токенизации и полученный результат от трёх процессов представляет в виде таблицы

        for method, tokens in results.items(): - здесь выводится каждый ключ словаря и его значение
            print(f"{method}: {tokens}")
            print('-'*10) - разграничение ввела для себя для более удобного просмотра


Инструкции по установлению были взяты с официальных сайтов NLTK и SpaCy:

    Для установления NLTK необходимо в терминале запустить эту команду:
    pip install nltk

    Для установления SpaCy необходимо последавательно в терминал вводить следующее: 
    (Я решила дополнительно установить пайплайн с русским языком, однако при выполнении задания, мне показалось, что это было не совсем обязательно. При тестовой попытке я токенизировала английский текст и прописала для него именно английский пайплайн -  spacy.load("en_core_web_sm"). Затем я токенизировала русский текст, но забыла изменить языковой пайплайн, но токенизация всё равно прошла успешно.)

    pip install -U pip setuptools wheel
    pip install -U spacy
    python -m spacy download en_core_web_sm
    python -m spacy download ru_core_news_sm
